# -*- coding: utf-8 -*-
"""Modelado Predictivo de la Tasa de Natalidad Trimestral Departamental.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_veJQzFOGQQtEhPC82IKqXFFPIhzlbhX

# **Modelado Predictivo de la Tasa de Natalidad Trimestral a Nivel Departamental**

Este notebook implementa el pipeline completo de Machine Learning para la predicción trimestral de tasas de natalidad departamental en el Perú. Incluye preprocesamiento avanzado de datos del Registro de Nacidos Vivos (2015-2025), ingeniería de características espacio-temporales, entrenamiento comparativo de múltiples algoritmos (Regresión Lineal, KNN, Árboles, Random Forest, XGBoost) y análisis exhaustivo de resultados. El modelo XGBoost demostró el mejor desempeño (R2=0.91), identificando patrones temporales (53.08%) y heterogeneidad geográfica (38.77%) como los principales predictores, con análisis detallado del impacto COVID-19 y características departamentales clave.

## **1. Configuración y Carga de datos**

### **1.1. Configuración inicial**
"""

from google.colab import drive
drive.mount('/content/drive')

# Librerías:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import math

"""### **1.2. Carga y preparación de datos**"""

# =============================================================================
# Fuente: Plataforma Nacional de Datos Abiertos del Perú
# Dataset: Registros de Nacidos Vivos en el Perú (2015-2025)
# Enlace: https://www.datosabiertos.gob.pe/dataset/registros-de-nacidos-vivos-en-el-per%C3%BA-2015%E2%80%932025
# =============================================================================

#Cargar el dataset y analizar filas dañadas
df_nacimientos = pd.read_csv('CNV_MINSA_4782338_CORTE_310825.csv', delimiter=';', on_bad_lines='skip',dtype={'Num_embar_madre': str})

# Diccionario de renombrado de columnas
df_nacimientos.rename(columns={
    'FecNac_Año': 'Año_Nacimiento',
    'FecNac_Mes': 'Mes_Nacimiento',
    'PESO_NACIDO': 'Peso_Nacido',
    'TALLA_NACIDO': 'Talla_Nacido',
    'DUR_EMB_PARTO': 'Duracion_Embarazo',
    'Condicion_Parto': 'Condicion_Parto',
    'sexo_nacido': 'Sexo_Nacido',
    'Tipo_Parto': 'Tipo_Parto',
    'Edad_Madre': 'Edad_Madre',
    'Estado_Civil': 'Estado_Civil_Madre',
    'Nivel_Intrucción_Madre': 'Nivel_Instruccion_Madre',
    'DESC_OCUPACION': 'Ocupacion_Madre',
    'Num_embar_madre': 'Numero_Embarazos_Madre',
    'Hijos_vivo_madre': 'Hijos_Vivos_Madre',
    'Hijos_fallec_madre': 'Hijos_Fallecidos_Madre',
    'nacmuer_abort_madre': 'Abortos_Madre',
    'Pais_Madre': 'Pais_Madre',
    'IdUbigeoInei': 'Codigo_Ubigeo',
    'Ipress': 'Ipress_Hospital',
    'Lugar_Nacido': 'Lugar_Nacimiento',
    'Atiende_Parto': 'Atiende_Parto',
    'Financiador_Parto': 'Financiador_Parto'
}, inplace=True)

# Vista preliminar de los datos
df_nacimientos.head(7)

"""### **1.3. Análisis exploratorio inicial**



"""

df_nacimientos.columns
df_nacimientos.dtypes

"""**Resultado:** Los valores corresponden a los esperados."""

df_nacimientos.shape

"""## **2. Preprocesamiento y Limpieza de datos**

### **2.1. Evaluación de valores duplicados**
"""

# Evaluar presencia de registros duplicados
total_registros = len(df_nacimientos)
duplicados = df_nacimientos.duplicated().sum()
porcentaje_duplicados = (duplicados / total_registros) * 100
porcentaje_duplicados_redondeado = round(porcentaje_duplicados, 3)

print(f"¿Existen filas duplicadas? {df_nacimientos.duplicated().any()}")
print(f"Número de filas duplicadas: {duplicados}")
print(f"Total de filas: {total_registros}")
print(f"Porcentaje de duplicados: {porcentaje_duplicados_redondeado}%")

"""**Resultado:** Sí hay valores duplicados, en total son 953 de las 4781957 filas. Las eliminaremos porque no aportan nueva información al modelo.

"""

# Eliminar duplicados para evitar sesgos en el análisis

df_nacimientos.drop_duplicates(inplace=True)

tot_reg_sin_dup = len(df_nacimientos)

print(f"Total de filas luego de eliminación: {tot_reg_sin_dup}")

"""### **2.2. Selección de features relevantes**


"""

df_nacimientos.columns

# Eliminar variables que no serán utilizadas en el análisis predictivo
# Estas características se descartan por no estar relacionadas con el objetivo del modelo

df_nacimientos.drop(columns=['Peso_Nacido'], inplace=True)
df_nacimientos.drop(columns=['Talla_Nacido'], inplace=True)
df_nacimientos.drop(columns=['Condicion_Parto'], inplace=True)
df_nacimientos.drop(columns=['Sexo_Nacido'], inplace=True)
df_nacimientos.drop(columns=['Tipo_Parto'], inplace=True)
df_nacimientos.drop(columns=['Duracion_Embarazo'], inplace=True)
df_nacimientos.drop(columns=['Ocupacion_Madre'], inplace=True)
df_nacimientos.drop(columns=['Ipress_Hospital'], inplace=True)
df_nacimientos.drop(columns=['Atiende_Parto'], inplace=True)
df_nacimientos.drop(columns=['Pais_Madre'], inplace=True)

print(f"Dataset luego de la eliminación: ")
df_nacimientos.head(10)
df_nacimientos.columns

"""### **2.3. Análisis de variables categóricas**

"""

# Explorar los valores únicos en cada columna categórica
cat_cols = df_nacimientos.select_dtypes(include=['object', 'category']).columns

for col in cat_cols:
    print(f"\nColumna categórica: {col}")
    print(df_nacimientos[col].unique())

"""### **2.4. Manejo de valores desconocidos (ignorados) o inválidos**"""

# Reemplazar valores marcados como 'IGNORADO' por NaN para tratamiento posterior
# Estado civil de la madre
df_nacimientos['Estado_Civil_Madre'] = df_nacimientos['Estado_Civil_Madre'].replace('IGNORADO', np.nan)
# Nivel de instrucción de la madre
df_nacimientos['Nivel_Instruccion_Madre'] = df_nacimientos['Nivel_Instruccion_Madre'].replace('IGNORADO', np.nan)


# Normalizar y convertir variables numéricas con valores especiales
# Número de embarazos de la madre
df_nacimientos['Numero_Embarazos_Madre'] = df_nacimientos['Numero_Embarazos_Madre'].replace({
    '-1': np.nan,
    '>=5': '5'
})
df_nacimientos['Numero_Embarazos_Madre'] = pd.to_numeric(df_nacimientos['Numero_Embarazos_Madre'], errors='coerce').astype('Int64')

# Número de hijos vivos de la madre
df_nacimientos['Hijos_Vivos_Madre'] = df_nacimientos['Hijos_Vivos_Madre'].replace({
    '-1': np.nan,
    '>=5': '5'
})
df_nacimientos['Hijos_Vivos_Madre'] = pd.to_numeric(df_nacimientos['Hijos_Vivos_Madre'], errors='coerce').astype('Int64')

# Número de hijos fallecidos de la madre
df_nacimientos['Hijos_Fallecidos_Madre'] = df_nacimientos['Hijos_Fallecidos_Madre'].replace({
    '-1': np.nan,
    '>=5': '5'
})
df_nacimientos['Hijos_Fallecidos_Madre'] = pd.to_numeric(df_nacimientos['Hijos_Fallecidos_Madre'], errors='coerce').astype('Int64')

# Número de abortos de la madre
df_nacimientos['Abortos_Madre'] = df_nacimientos['Abortos_Madre'].replace({
    'NINGUNO': '0',
    '11 a más': '11',
    'IGNORADO': np.nan
})
# Convertir a número (permitiendo NaN)
df_nacimientos['Abortos_Madre'] = pd.to_numeric(df_nacimientos['Abortos_Madre'], errors='coerce').astype('Int64')


# Limpiar otras variables categóricas
# Lugar de nacimiento
df_nacimientos['Lugar_Nacimiento'] = df_nacimientos['Lugar_Nacimiento'].replace('IGNORADO', np.nan)

#Financiador del parto
df_nacimientos['Financiador_Parto'] = df_nacimientos['Financiador_Parto'].replace('IGNORADO', np.nan)

# Verificar tipos de datos después de las conversiones
for col in df_nacimientos.columns:
    if pd.api.types.is_numeric_dtype(df_nacimientos[col]):
        print(f"Columna numérica: {col}")
    else:
        print(f"Columna categórica: {col}")

"""### **2.5. Análisis de valores nulos**"""

# Evaluar el porcentaje de valores nulos por columna
print("Porcentaje de valores nulos encontrados en total: \n", 100*df_nacimientos.isnull().sum() /df_nacimientos.shape[0])

"""**Resultado:** Se decide dropear la columna Hijos_Fallecidos_Madre ya que tiene un alto porcentaje de nulos (96.97%)."""

# Eliminar columna con alto porcentaje de valores nulos (96.97%)
df_nacimientos.drop(columns=['Hijos_Fallecidos_Madre'], inplace=True)

"""### **2.6. Análisis de distribución de variables**

Analizamos histogramas y frecuencias de los features.
"""

# Visualizar distribuciones de variables numéricas
df_nacimientos.hist(figsize=(15,8),layout=[2,4])
plt.show()

# Visualizar distribuciones de variables categóricas
cat_cols = df_nacimientos.select_dtypes(include=['object', 'category']).columns

# Preparar los gráficos
cols_per_row = 2
n_cols = len(cat_cols)
n_rows = math.ceil(n_cols / cols_per_row)

plt.figure(figsize=(cols_per_row * 12, n_rows * 6))

for idx, col in enumerate(cat_cols, 1):
    plt.subplot(n_rows, cols_per_row, idx)

    counts = df_nacimientos[col].value_counts()

    plt.barh(counts.index, counts.values)
    plt.title(col)
    plt.xlabel("Frecuencia")
    plt.tight_layout()

plt.show()

"""### **2.7. Detección de outliers y valores fuera de rango**

"""

# Analizar posibles valores atípicos mediante boxplots
df_nacimientos.plot(kind='box',subplots=True,sharex=True,sharey=False,layout=[2,4],figsize=(15,8))
plt.show()

"""**Resultado:** Analizando los boxplots, vemos que existen outliers, pero estos datos están dentro de los valores esperados según la data. Por ejemplo, para edad madre existen casos de madres cuya edad supere los 40 años. Para abortos madre, está bien mapeado los datos ya que se tienen valores de 0 a 11, y eso muestra su boxplot.

Como tenemos data importante que luego se agregará en trimestres, no haremos la imputación de nulos. Estos se ignorarán al momento de convertir las categóricas a porcentajes y las numèricas a medias
"""

df_nacimientos.head(8)

cols = df_nacimientos.columns
cols

"""### **2.8. Agrupación de categorías**


"""

# Revisar categorías actuales antes de agrupar
cat_cols = df_nacimientos.select_dtypes(include=['object', 'category']).columns

for col in cat_cols:
    print(f"\nColumna categórica: {col}")
    print(df_nacimientos[col].unique())

"""Para el estado civil de la madre:"""

# Agrupar estado civil en categorías más generales
map_estado_civil = {
    'CASADO': 'CON_PAREJA',
    'CONVIVIENTE': 'CON_PAREJA',
    'SOLTERO': 'SIN_PAREJA',
    'DIVORCIADO': 'SIN_PAREJA',
    'SEPARADO': 'SIN_PAREJA',
    'VIUDO': 'SIN_PAREJA'
}

df_nacimientos['Estado_Civil_Agrupado'] = df_nacimientos['Estado_Civil_Madre'].map(map_estado_civil)
# Los NaN se mantienen como NaN

"""Para el nivel de instrucción de la madre:"""

# Agrupar nivel de instrucción en categorías simplificadas
map_nivel_instr = {
    # Superior
    'SUPERIOR UNIV. COMPLETA': 'SUPERIOR',
    'SUPERIOR UNIV. INCOMPLETA': 'SUPERIOR',
    'SUPERIOR NO UNIV. COMPLETA': 'SUPERIOR',
    'SUPERIOR NO UNIV. INCOMPLETA': 'SUPERIOR',

    # Secundaria
    'SECUNDARIA COMPLETA': 'SECUNDARIA',
    'SECUNDARIA INCOMPLETA': 'SECUNDARIA',

    # Primaria / baja escolaridad
    'PRIMARIA COMPLETA': 'PRIMARIA_BAJA',
    'PRIMARIA INCOMPLETA': 'PRIMARIA_BAJA',
    'NINGUN NIVEL/ILETRADO': 'PRIMARIA_BAJA',
    'INCIAL/PRE-ESCOLAR': 'PRIMARIA_BAJA'
}

df_nacimientos['Nivel_Instruccion_Agrupado'] = df_nacimientos['Nivel_Instruccion_Madre'].map(map_nivel_instr)
# De nuevo, NaN se queda como NaN

"""Para el lugar de nacimiento:"""

# Normalizar y agrupar lugar de nacimiento
df_nacimientos['Lugar_Nacimiento'] = (
    df_nacimientos['Lugar_Nacimiento']
        .astype('string')
        .str.strip()
        .str.upper()
)

# Convertir la string "nan" que pudo aparecer al hacer astype('string') en NA real
df_nacimientos['Lugar_Nacimiento'] = df_nacimientos['Lugar_Nacimiento'].replace('NAN', pd.NA)

# Crear la nueva columna inicializada en NA (tipo string/categorical luego)
df_nacimientos['Lugar_Nacimiento_Agrupado'] = pd.NA

# Marcar INSTITUCIONAL
mask_inst = df_nacimientos['Lugar_Nacimiento'] == 'ESTABLECIMIENTO DE SALUD'
df_nacimientos.loc[mask_inst, 'Lugar_Nacimiento_Agrupado'] = 'INSTITUCIONAL'

# Todo lo que NO es institucional pero NO es nulo se vuelve NO_INSTITUCIONAL
mask_no_inst = df_nacimientos['Lugar_Nacimiento'].notna() & df_nacimientos['Lugar_Nacimiento_Agrupado'].isna()
df_nacimientos.loc[mask_no_inst, 'Lugar_Nacimiento_Agrupado'] = 'NO_INSTITUCIONAL'

# Pasar a categórica
df_nacimientos['Lugar_Nacimiento_Agrupado'] = df_nacimientos['Lugar_Nacimiento_Agrupado'].astype('category')

"""Para el financiador del parto:"""

# Agrupar financiador del parto
map_financiador = {
    'SIS': 'SIS',
    'ESSALUD': 'ESSALUD',
    'SANIDAD FAP': 'SANIDADES',
    'SANIDAD NAVAL': 'SANIDADES',
    'SANIDAD PNP': 'SANIDADES',
    'SANIDAD EP': 'SANIDADES',
    'PARTICULAR': 'PRIVADO',
    'PRIVADOS': 'PRIVADO'
}

df_nacimientos['Financiador_Agrupado'] = df_nacimientos['Financiador_Parto'].map(map_financiador)

"""### **2.9. Eliminación de columnas originales**"""

# Eliminar columnas originales que fueron reemplazadas por versiones agrupadas
cols_to_drop = [
    'Estado_Civil_Madre',
    'Nivel_Instruccion_Madre',
    'Lugar_Nacimiento',
    'Financiador_Parto'
]

df_nacimientos = df_nacimientos.drop(columns=cols_to_drop)

# Vista final del dataset procesado
df_nacimientos.head()

"""## **3. Preparación final del Dataset**

### **3.1. Integración con datos geográficos (Ubigeos)**
"""

# =============================================================================
# Fuente: Geodir / INEI
# Dataset: Catálogo de Ubigeo del INEI (2019)
# Enlace: https://account.geodir.co/recursos/ubigeo-inei-peru.html
# =============================================================================

# Cargar dataset de ubigeos para obtener departamentos
df_ubigeos = pd.read_csv("Lista_Ubigeos_INEI.csv", delimiter=';')
df_ubigeos.head()

# Realizar merge para enriquecer los datos con los departamentos respectivos
df_nacimientos = df_nacimientos.merge(
    df_ubigeos[["UBIGEO_INEI", "DEPARTAMENTO"]],
    left_on="Codigo_Ubigeo",
    right_on="UBIGEO_INEI",
    how="left"
)

# Renombrar la nueva columna
df_nacimientos = df_nacimientos.rename(columns={"DEPARTAMENTO": "Departamento"})

# Eliminar columnas redundantes después del merge (Codigo_Ubigeo y UBIGEO_INEI)
df_nacimientos = df_nacimientos.drop(columns=["Codigo_Ubigeo", "UBIGEO_INEI"])
df_nacimientos.head(5)

"""### **3.2. Transformación de variables temporales**

Formamos columna de trimestre, renombramos y ordenamos.
"""

# Crear columna de trimestre a partir del mes de nacimiento
df_nacimientos['Trimestre'] = ((df_nacimientos['Mes_Nacimiento'] - 1) // 3 + 1).astype('Int64')

# Renombrar columna de año
df_nacimientos = df_nacimientos.rename(columns={'Año_Nacimiento': 'Anho'})

# Eliminar columna de mes ya que ahora tenemos trimestre
df_nacimientos = df_nacimientos.drop(columns=['Mes_Nacimiento'])

# Verificar porcentaje de valores nulos en el dataset después de las transformaciones
print("Porcentaje de valores nulos encontrados en total: \n", 100*df_nacimientos.isnull().sum() /df_nacimientos.shape[0])

# Eliminar registros sin departamento asignado
df_nacimientos = df_nacimientos.dropna(subset=['Departamento'])

print("Porcentaje de valores nulos luego del dropeo: \n", 100*df_nacimientos.isnull().sum() /df_nacimientos.shape[0])

"""### **3.3. Reorganización del dataset**"""

# Definir columnas base
cols_base = ['Anho', 'Trimestre', 'Departamento']

# Obtener todas las demás columnas
cols_rest = [c for c in df_nacimientos.columns if c not in cols_base]

# Reordenar DataFrame
df_nacimientos = df_nacimientos[cols_base + cols_rest]

df_nacimientos.head(7)

# Verificar columnas finales
cols_df = df_nacimientos.columns
cols_df

"""## **4. Agregación y Construcción del Dataset para modelado**

### **4.1. Agregación por año y trimestre**
"""

# Definir columnas para agrupar
group_cols = ['Departamento', 'Anho', 'Trimestre']
g = df_nacimientos.groupby(group_cols)

# Crear dataset base con conteo de nacimientos
df_panel = (
    g.size()
     .rename('Nacimientos')
     .reset_index()
)

"""### **4.2. Cálculo de variables de la edad materna**"""

# Calcular estadísticas agregadas de la edad de la madre
edad_agg = g['Edad_Madre'].agg(
    Edad_Media = 'mean',
    Pct_Adolescentes = lambda s: ( (s < 20).sum() / s.notna().sum() ) if s.notna().sum() > 0 else np.nan,
    Pct_Mayores35   = lambda s: ( (s >= 35).sum() / s.notna().sum() ) if s.notna().sum() > 0 else np.nan
)

edad_agg = edad_agg.reset_index()
df_panel = df_panel.merge(edad_agg, on=group_cols, how='left')

"""### **4.3. Cálculo de variables de la historia reproductiva**"""

# Calcular promedios de las variables reproductivas
repro_agg = g[['Numero_Embarazos_Madre', 'Hijos_Vivos_Madre', 'Abortos_Madre']].agg('mean')
repro_agg = repro_agg.rename(columns={
    'Numero_Embarazos_Madre': 'Embarazos_Prom',
    'Hijos_Vivos_Madre': 'HijosVivos_Prom',
    'Abortos_Madre': 'Abortos_Prom'
}).reset_index()

df_panel = df_panel.merge(repro_agg, on=group_cols, how='left')

"""### **4.4. Cálculo de proporciones categóricas**"""

# Función para calcular proporciones departamentales
def calcular_proporciones_departamentales(g, group_cols, columna_agrupada, prefix):
    prop_departamental = (
        g[columna_agrupada]
        .value_counts(normalize=True)
        .rename('valor')
        .rename_axis(index=group_cols + [columna_agrupada])
        .reset_index()
        .pivot(index=group_cols,
               columns=columna_agrupada,
               values='valor')
        .add_prefix(prefix)
        .reset_index()
    )
    return prop_departamental

# Calcular todas las proporciones departamentales
estado_prop = calcular_proporciones_departamentales(g, group_cols, 'Estado_Civil_Agrupado', 'Pct_EstadoCivil_')
nivel_prop = calcular_proporciones_departamentales(g, group_cols, 'Nivel_Instruccion_Agrupado', 'Pct_Nivel_')
lugar_prop = calcular_proporciones_departamentales(g, group_cols, 'Lugar_Nacimiento_Agrupado', 'Pct_Lugar_')
finan_prop = calcular_proporciones_departamentales(g, group_cols, 'Financiador_Agrupado', 'Pct_Financ_')

# Unir todas las proporciones al dataset panel
df_panel = df_panel.merge(estado_prop, on=group_cols, how='left')
df_panel = df_panel.merge(nivel_prop, on=group_cols, how='left')
df_panel = df_panel.merge(lugar_prop, on=group_cols, how='left')
df_panel = df_panel.merge(finan_prop, on=group_cols, how='left')

"""### **4.5. Imputación de valores en proporciones**"""

# Identificar columnas proporcionales (empiezan con Pct_)
pct_cols = [c for c in df_panel.columns if c.startswith('Pct_')]

# Imputar 0 solo en columnas proporcionales
df_panel[pct_cols] = df_panel[pct_cols].fillna(0)

df_panel.head()

"""## **5. Integración con la población y Cálculo de la tasa**

### **5.1. Carga y preparación de datos de población**
"""

# =============================================================================
# Fuente: Instituto Nacional de Estadística e Informática (INEI)
# Dataset: Proyecciones de Población por Departamento
# Enlace: https://m.inei.gob.pe/estadisticas/indice-tematico/population-estimates-and-projections/
# =============================================================================

# Cargar dataset de poblaciones trimestrales
df_poblacion = pd.read_csv('poblaciones_trimestrales_con_tasas.csv')
# Cargar dataset de poblaciones trimestrales
df_poblacion = pd.read_csv('poblaciones_trimestrales_con_tasas.csv')

# Convertir trimestre de formato "T1" a uno numérico
df_poblacion['Trimestre'] = df_poblacion['Trimestre'].str.replace('T', '').astype(int)

# Renombrar columna
df_poblacion = df_poblacion.rename(columns={'Año': 'Anho'})

# Función para normalizar nombres de departamentos
def normalizar_departamento(nombre):
    nombre = nombre.upper().strip()
    # Eliminar tildes y caracteres especiales
    reemplazos = {
        'Á': 'A', 'É': 'E', 'Í': 'I', 'Ó': 'O', 'Ú': 'U',
        'Ü': 'U', 'Ñ': 'N'
    }
    for viejo, nuevo in reemplazos.items():
        nombre = nombre.replace(viejo, nuevo)

    # Caso especial para Callao
    if 'CALLAO' in nombre:
        return 'CALLAO'

    return nombre

# Aplicar normalización
df_poblacion['Departamento'] = df_poblacion['Departamento'].apply(normalizar_departamento)

"""### **5.2. Unión con datos de población**"""

# Realizar merge con los datos de población
df_panel = df_panel.merge(
    df_poblacion[['Anho', 'Trimestre', 'Departamento', 'Poblacion_Trimestral']],
    on=['Departamento', 'Anho', 'Trimestre'],
    how='left'
)

df_panel.head()

"""### **5.3. Cálculo de la tasa de natalidad**"""

# Calcular tasa de natalidad trimestral con población total
df_panel['Tasa_Natalidad_Trimestral'] = (
    df_panel['Nacimientos'] / df_panel['Poblacion_Trimestral']
)
# Por 1000 habitantes
df_panel['Tasa_Natalidad_Trimestral'] *= 1000

df_panel.head()

"""### **5.4. Verificación de dimensiones finales**"""

print(f"Dataset departamental: {df_panel.shape}")
print("Valores nulos en tasa departamental:", df_panel['Tasa_Natalidad_Trimestral'].isnull().sum())

"""## **6. Análisis exploratorio del Dataset final**

### **6.1. Análisis de distribución de variables**
"""

# Visualizar distribuciones de todas las variables numéricas
df_panel.hist(figsize=(15,15),layout=(5,5))
plt.suptitle('Distribución de Variables - Nivel Departamental', y=0.95)
plt.show()

"""### **6.2. Análisis de correlaciones entre variables**

Graficamos las matrices de correlación.
"""

# Seleccionar solo variables numéricas
df_corr = df_panel.select_dtypes(include=[np.number])

# Calcular matriz de correlación absoluta
corr = df_corr.corr().abs() # es más útil para identificar relaciones fuertes

# Visualizar matriz de correlación
plt.figure(figsize=(16, 12))
sns.heatmap(
    corr,
    annot=True,
    cmap='Blues',
    linewidths=0.5
)
plt.title('Matriz de Correlación - Panel Trimestral Departamental')
plt.show()

"""**Resultado:** Identificamos variables con alta correlación (mayor o igual a 0.9), como Nacimiento y Poblacion_Trimestral (0.97).

### **6.3. Identificación y manejo de variables altamente correlacionadas**
"""

# Definir umbral para considerar correlación alta
umbral = 0.9

# Extraer pares de variables correlacionadas
# Solo el triángulo superior para evitar duplicados
corr_pairs = (
    corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))
        .stack()
        .reset_index()
)

corr_pairs.columns = ['Variable_1', 'Variable_2', 'Correlacion']

# Filtrar pares con correlación por encima del umbral
corr_altas = corr_pairs[corr_pairs['Correlacion'] >= umbral]\
             .sort_values(by='Correlacion', ascending=False)

corr_altas

# Eliminar variables redundantes identificadas en el análisis de correlación
df_final = df_panel.drop(columns=['Pct_Lugar_NO_INSTITUCIONAL', 'Pct_EstadoCivil_SIN_PAREJA', 'Nacimientos'])

df_final.shape

"""Se eliminaron *Pct_Lugar_NO_INSTITUCIONAL*, *Pct_EstadoCivil_SIN_PAREJA* y *Nacimientos* por presentar correlaciones perfectas (1.0) o muy altas (0.97) que indican redundancia informativa y riesgo de multicolinealidad en los modelos predictivos.

### **6.4. Exportación del dataset final**
"""

# Guardar y descargar el dataset final procesado
from google.colab import files

# Exportar dataset departamental
df_final.to_csv('dataset_final_natalidad.csv', index=False)
files.download('dataset_final_natalidad.csv')

print("Dataset exportado:")
print(" - dataset_final_natalidad.csv")

# Verificar dimensión final

print(f"Dataset departamental: {df_final.shape}")

"""### **6.5. Matriz de correlación del dataset final**"""

# Recalcular matriz de correlación después de eliminar variables redundantes
df_corr2 = df_final.select_dtypes(include=[np.number])
corr = df_corr2.corr().abs()

# Visualizar nueva matriz de correlación
plt.figure(figsize=(16, 12))
sns.heatmap(
    corr,
    annot=True,
    cmap='Blues',
    linewidths=0.5
)
plt.title('Matriz de Correlación - Dataset Departamental Final')
plt.show()

"""### **6.6. Análisis de comportamiento temporal de la tasa de natalidad**"""

# Mapeo de trimestres a meses (primer mes de cada trimestre)
mes_trimestre = {1:1, 2:4, 3:7, 4:10}

# Preparar datos para análisis temporal departamental
df_aux = df_final[[
    "Anho",
    "Trimestre",
    "Departamento",
    "Tasa_Natalidad_Trimestral"
]].copy()

# Crear columna de fecha a partir de Año + Trimestre para el análisis temporal
df_aux["Fecha"] = pd.to_datetime(
    df_aux["Anho"].astype(str) + "-" +
    df_aux["Trimestre"].map(mes_trimestre).astype(str) + "-01"
)

# Ordenar datos por departamento y fecha
df_aux = df_aux.sort_values(["Departamento", "Fecha"])

df_aux.head()

"""### **6.7. Visualización de series temporales**"""

# Generar gráficas individuales para cada departamento
for depto in df_aux["Departamento"].unique():
    # Filtrar datos del departamento actual
    data = df_aux[df_aux["Departamento"] == depto]

    # Crear figura para el departamento
    plt.figure(figsize=(12, 6))
    plt.plot(
        data["Fecha"],
        data["Tasa_Natalidad_Trimestral"],
        marker="o",
        linewidth=2,
        markersize=4
    )

    plt.title(f"Evolución de la Tasa de Natalidad Trimestral – {depto}")
    plt.xlabel("Fecha")
    plt.ylabel("Tasa de Natalidad (por 1000 habitantes)")
    plt.grid(True, linestyle="--", alpha=0.3)

    plt.tight_layout()
    plt.show()

"""### **6.8. Ingeniería de features temporales**

Crearemos variables temporales para capturar patrones estacionales y tendencias. Estas features ayudarán a los modelos a aprender patrones temporales.


*   **Lag 1:** Valor del trimestre anterior
*   **Lag 4:** Valor del mismo trimestre del año anterior (estacionalidad anual)
*   **Media móvil de 4 trimestres (ma4):** Tendencia suavizada
"""

# Crear variables temporales departamentales
df_final['lag1'] = df_final.groupby('Departamento')['Tasa_Natalidad_Trimestral'].shift(1)
df_final['lag4'] = df_final.groupby('Departamento')['Tasa_Natalidad_Trimestral'].shift(4)
df_final['ma4']  = df_final.groupby('Departamento')['Tasa_Natalidad_Trimestral'].shift(1).rolling(4).mean()

# Imputar NaN
df_final = df_final.fillna(0)

df_final.head(5)

print("Resumen del dataset final con features temporales:")
print(f"Dimensiones: {df_final.shape}")
print(f"Columnas: {list(df_final.columns)}")

"""## **7. Modelado Predictivo**

### **7.1. Configuración de librerías y entornos**
"""

# Librerías para modelos de regresión
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet # para modelos de regresion lineal, Ridge regression, Lasso regression, ElasticNet regression
from sklearn.neighbors import KNeighborsRegressor # Knn regression
from sklearn.tree import DecisionTreeRegressor    # Decision Trees regression
from sklearn.svm import SVR                       # Support Vector regression
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor

# Librerías para preprocesamiento y evaluación
from sklearn.model_selection import train_test_split  # para partir los datos en conjuntos de entrenamiento y validacion
from sklearn.model_selection import KFold  # para partir la data en k-folds
from sklearn.model_selection import TimeSeriesSplit, cross_val_score # para evaluar algoritmos en cross validacion
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.metrics import mean_squared_error, mean_absolute_error # para manejar metricas de desempeño
from sklearn.metrics import explained_variance_score  # para hacer reportes de resultados de clasificacion
from sklearn.metrics import r2_score  # para manejar matrices de confusion

"""### **7.2. Preparación de datos para modelado**"""

# Ordenar por departamento, año y trimestre
df_final = df_final.sort_values(['Departamento', 'Anho', 'Trimestre'])\
                   .reset_index(drop=True)

# Filtrar datos recientes (excluir 2025-T3 por datos insuficientes)
df_final = df_final[(df_final['Anho'] < 2025) | ((df_final['Anho'] == 2025) & (df_final['Trimestre'] <= 2))]

# Convertir variable Departamento a dummies (one-hot encoding)
if df_final["Departamento"].dtype == "object":
    df_final_dummies = pd.get_dummies(df_final, columns=["Departamento"], drop_first=True)

# Sin covid
# Filtrar periodo COVID (2020 y primeros trimestres 2021)
mask_covid = (
    (df_final_dummies['Anho'] == 2020) |
    ((df_final_dummies['Anho'] == 2021) & (df_final['Trimestre'] <= 2))
)

df_model_sin_covid = df_final_dummies.loc[~mask_covid].copy()

# Asegurar orden temporal
df_model_sin_covid = df_model_sin_covid.sort_values(['Anho', 'Trimestre']).reset_index(drop=True)

# Con covid
df_model_con_covid = df_final_dummies.copy()
df_model_con_covid = df_model_con_covid.sort_values(['Anho', 'Trimestre']).reset_index(drop=True)

print(f"Dataset departamental para modelado (sin covid): {df_model_sin_covid.shape}")
print(f"Dataset departamental para modelado (con covid): {df_model_con_covid.shape}")

# Seleccionar uno para el experimento actual:
df_model = df_model_sin_covid.copy() # Para experimento SIN COVID
# df_model = df_model_con_covid.copy() # Para experimento CON COVID

"""Seleccionamos el modelo sin el periodo de COVID.

### **7.3. División temporal de datos**
"""

# Definir límites temporales
ANHO_TRAIN_END = 2022
ANHO_VALID_END = 2023

# Conjunto de entrenamiento (hasta 2022)
df_train = df_model[df_model['Anho'] <= ANHO_TRAIN_END]

# Conjunto de validación (2023)
df_valid = df_model[
    (df_model['Anho'] > ANHO_TRAIN_END) &
    (df_model['Anho'] <= ANHO_VALID_END)
]

# Conjunto de prueba (posterior a 2023)
df_test = df_model[df_model['Anho'] > ANHO_VALID_END]

# Definir variables objetivo y características
target_col = 'Tasa_Natalidad_Trimestral'
feature_cols = [c for c in df_model.columns if c != target_col] # Todas las columnas excepto el target

X_train = df_train[feature_cols]
y_train = df_train[target_col]

X_valid = df_valid[feature_cols]
y_valid = df_valid[target_col]

X_test  = df_test[feature_cols]
y_test  = df_test[target_col]

print(f"Division departamental - Train: {X_train.shape}, Valid: {X_valid.shape}, Test: {X_test.shape}")

"""### **7.4. Configuración de validación cruzada temporal**"""

# Configurar validación cruzada temporal (5 folds)
tscv = TimeSeriesSplit(n_splits=5)

print("Estructura de TimeSeriesSplit - Departamental:")
for fold, (train_idx, valid_idx) in enumerate(tscv.split(X_train), start=1):
    print(f"\nFold {fold}")
    print("Train:", X_train.index[train_idx][0], "->", X_train.index[train_idx][-1])
    print("Valid:", X_train.index[valid_idx][0], "->", X_train.index[valid_idx][-1])

"""### **7.5. Definición de pipeline de modelos**"""

cols_no_scalar = ['Anho', 'Trimestre']
cols_scalar = [c for c in X_train.columns if c not in cols_no_scalar]

ct = ColumnTransformer([
    ('scaling', StandardScaler(), cols_scalar)
], remainder='passthrough')

modelos = []

# ---- Regresión Lineal ----
modelos.append((
    "Regresión Lineal",
    Pipeline([
        ("scaler", ct),
        ("model", LinearRegression())
    ])
))

# ---- KNN ----
modelos.append((
    "KNN (k=5)",
    Pipeline([
        ("scaler", ct),
        ("model", KNeighborsRegressor(n_neighbors=5))
    ])
))

modelos.append((
    "KNN (k=10)",
    Pipeline([
        ("scaler", ct),
        ("model", KNeighborsRegressor(n_neighbors=10))
    ])
))

# ---- Árbol de Decisión ----
modelos.append((
    "Árbol (max_depth=6)",
    Pipeline([
        ("scaler", ct),
        ("model", DecisionTreeRegressor(max_depth=6, random_state=42))
    ])
))

# ---- Random Forest ----
modelos.append((
    "Random Forest (100 árboles)",
    Pipeline([
        ("scaler", ct),
        ("model", RandomForestRegressor(
            n_estimators=100,
            max_depth=6,
            random_state=42
        ))
    ])
))

# ---- XGBoost ----
modelos.append((
    "XGBoost",
    Pipeline([
        ("scaler", ct),
        ("model", XGBRegressor(
            n_estimators=200,
            learning_rate=0.05,
            max_depth=6,
            subsample=0.8,
            colsample_bytree=0.8,
            objective="reg:squarederror",
            random_state=42
        ))
    ])
))

"""### **7.6. Entrenamiento y validación cruzada**"""

resultados = {
    'R2': {},
    'RMSE': {},
    'MAE': {}
}  # Diccionarios anidados para cada métrica
nombres = []

print("VALIDACIÓN CRUZADA - DEPARTAMENTAL\n")
for nombre, modelo in modelos:
    nombres.append(nombre)
    r2s = []
    rmses = []
    maes = []

    for fold, (idx_tr, idx_val) in enumerate(tscv.split(X_train), start=1):
        X_tr, X_val = X_train.iloc[idx_tr], X_train.iloc[idx_val]
        y_tr, y_val = y_train.iloc[idx_tr], y_train.iloc[idx_val]

        modelo.fit(X_tr, y_tr)
        y_pred = modelo.predict(X_val)

        # Cálculo de métricas
        r2s.append(r2_score(y_val, y_pred))
        rmses.append(np.sqrt(mean_squared_error(y_val, y_pred)))
        maes.append(mean_absolute_error(y_val, y_pred))

    # Almacenar resultados
    resultados['R2'][nombre] = r2s
    resultados['RMSE'][nombre] = rmses
    resultados['MAE'][nombre] = maes
    # Reportar promedios
    print(f"{nombre}:")
    print(f"  R2 promedio   = {np.mean(r2s):.4f}")
    print(f"  RMSE promedio = {np.mean(rmses):.4f}")
    print(f"  MAE promedio  = {np.mean(maes):.4f}\n")

# Visualización resultados departamental

# R2
plt.figure(figsize=(10, 6))

metrica = 'R2'
datos_box = [resultados[metrica][n] for n in nombres]

plt.boxplot(datos_box, labels=nombres)
plt.ylabel(f"{metrica} (más alto es mejor)")
plt.title(f"Comparación de Modelos - {metrica} (TimeSeriesSplit - 5 folds)")
plt.xticks(rotation=20, ha="right")
plt.tight_layout()
plt.show()

# RMSE
plt.figure(figsize=(10, 6))

metrica = 'RMSE'
datos_box = [resultados[metrica][n] for n in nombres]

plt.boxplot(datos_box, labels=nombres)
plt.ylabel(f"{metrica} (más bajo es mejor)")
plt.title(f"Comparación de Modelos - {metrica} (TimeSeriesSplit - 5 folds)")
plt.xticks(rotation=20, ha="right")
plt.tight_layout()
plt.show()

# MAE
plt.figure(figsize=(10, 6))

metrica = 'MAE'  # ¡CORREGIDO! Cambié 'RMSE' por 'MAE'
datos_box = [resultados[metrica][n] for n in nombres]

plt.boxplot(datos_box, labels=nombres)
plt.ylabel(f"{metrica} (más bajo es mejor)")
plt.title(f"Comparación de Modelos - {metrica} (TimeSeriesSplit - 5 folds)")
plt.xticks(rotation=20, ha="right")
plt.tight_layout()
plt.show()

"""### **7.7. Entrenamiento del modelol final**"""

# Unir train + valid para entrenar el modelo final
X_train_full = pd.concat([X_train, X_valid], axis=0)
y_train_full = pd.concat([y_train, y_valid], axis=0)

# Definir el modelo XGBoost final
xgb_final = XGBRegressor(
    n_estimators=600,
    learning_rate=0.05,
    max_depth=8,
    subsample=0.8,
    colsample_bytree=0.8,
    objective="reg:squarederror",
    random_state=42
)

# Entrenamiento
xgb_final.fit(X_train_full, y_train_full)

# Predicción en el set de test
y_pred_test = xgb_final.predict(X_test)

# Evaluación
r2 = r2_score(y_test, y_pred_test)
rmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))
mae_test = mean_absolute_error(y_test, y_pred_test)

print("RESULTADOS FINALES - DEPARTAMENTAL\n")
print(f"R2 score: {r2:.4f}")
print(f"RMSE en test: {rmse_test:.4f}")
print(f"MAE en test: {mae_test:.4f}")

"""### **7.8. Visualización de resultados**"""

# Preparar datos para gráficas departamentales

# Copia del df_test para armar la gráfica
df_plot = df_test.copy()

# Guardar los valores reales y predichos
df_plot['Tasa_Real'] = y_test.values
df_plot['Tasa_Pred'] = y_pred_test

# Crear una columna de periodo tipo "2018-T1", "2018-T2", etc
df_plot['Periodo'] = (
    df_plot['Anho'].astype(str) + '-T' + df_plot['Trimestre'].astype(str)
)

# Ordenar por tiempo
df_plot = df_plot.sort_values(['Anho', 'Trimestre']).reset_index(drop=True)

# Agregar por periodo (promedio de departamentos en cada trimestre)
df_agg = df_plot.groupby(['Anho', 'Trimestre'], as_index=False)[['Tasa_Real', 'Tasa_Pred']].mean()
df_agg['Periodo'] = df_agg['Anho'].astype(str) + '-T' + df_agg['Trimestre'].astype(str)

# Gráfico de líneas comparativo
plt.figure(figsize=(12, 5))

plt.plot(df_agg['Periodo'], df_agg['Tasa_Real'], marker='o', label='Real')
plt.plot(df_agg['Periodo'], df_agg['Tasa_Pred'], marker='o', label='Predicho')
plt.xticks(rotation=45, ha='right')
plt.ylabel('Tasa de natalidad trimestral')
plt.title('Tasa de natalidad real vs predicha (promedio por trimestre)')
plt.legend()
plt.tight_layout()
plt.show()

# Gráfica de dispersión
plt.figure(figsize=(6, 6))
plt.scatter(df_plot['Tasa_Real'], df_plot['Tasa_Pred'], alpha=0.6)
min_val = min(df_plot['Tasa_Real'].min(), df_plot['Tasa_Pred'].min())
max_val = max(df_plot['Tasa_Real'].max(), df_plot['Tasa_Pred'].max())
plt.plot([min_val, max_val], [min_val, max_val], 'r--', label='y = x')

plt.xlabel('Tasa real')
plt.ylabel('Tasa predicha')
plt.title('Real vs predicho en test (XGBoost)')
plt.legend()
plt.tight_layout()
plt.show()

"""### **7.9. Resumen final**"""

print("RESUMEN - MODELADO PREDICTIVO")

print("\n NIVEL DEPARTAMENTAL:")
print(f"  - R² Score: {r2:.4f}")
print(f"  - RMSE Test: {rmse_test:.4f}")
print(f"  - MAE Test: {mae_test:.4f}")
print(f"  - Observaciones en Test: {len(X_test)}")

"""### **7.10. Comparación COVID vs No-COVID**"""

def entrenar_y_evaluar(df_model, nombre_experimento):

    # División temporal (misma lógica que antes)
    ANHO_TRAIN_END = 2022
    ANHO_VALID_END = 2023

    df_train = df_model[df_model['Anho'] <= ANHO_TRAIN_END]
    df_valid = df_model[
        (df_model['Anho'] > ANHO_TRAIN_END) &
        (df_model['Anho'] <= ANHO_VALID_END)
    ]
    df_test = df_model[df_model['Anho'] > ANHO_VALID_END]

    # Preparar features y target
    X_train = df_train[feature_cols]
    y_train = df_train[target_col]
    X_valid = df_valid[feature_cols]
    y_valid = df_valid[target_col]
    X_test = df_test[feature_cols]
    y_test = df_test[target_col]

    print(f"\n{'='*50}")
    print(f"EXPERIMENTO: {nombre_experimento}")
    print(f"{'='*50}")
    print(f"Periodo de datos: {df_model['Anho'].min()}-{df_model['Anho'].max()}")
    print(f"Train: {X_train.shape}, Valid: {X_valid.shape}, Test: {X_test.shape}")

    # Entrenar y evaluar XGBoost (mejor modelo)
    xgb_model = XGBRegressor(
        n_estimators=600,
        learning_rate=0.05,
        max_depth=8,
        subsample=0.8,
        colsample_bytree=0.8,
        objective="reg:squarederror",
        random_state=42
    )

    # Entrenar con train + validation
    X_train_full = pd.concat([X_train, X_valid], axis=0)
    y_train_full = pd.concat([y_train, y_valid], axis=0)

    xgb_model.fit(X_train_full, y_train_full)
    y_pred = xgb_model.predict(X_test)

    # Métricas
    r2 = r2_score(y_test, y_pred)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    mae = mean_absolute_error(y_test, y_pred)

    print(f"\nRESULTADOS {nombre_experimento}:")
    print(f"R2: {r2:.4f}")
    print(f"RMSE: {rmse:.4f}")
    print(f"MAE: {mae:.4f}")

    return {
        'nombre': nombre_experimento,
        'r2': r2,
        'rmse': rmse,
        'mae': mae,
        'modelo': xgb_model,
        'X_test': X_test,
        'y_test': y_test,
        'y_pred': y_pred
    }

# Ejecutar ambos experimentos
resultado_sin_covid = entrenar_y_evaluar(df_model_sin_covid, "SIN COVID")
resultado_con_covid = entrenar_y_evaluar(df_model_con_covid, "CON COVID")

# Comparativa
print("COMPARATIVA FINAL: COVID vs NO-COVID")

comparativa = pd.DataFrame([
    resultado_sin_covid,
    resultado_con_covid
])[['nombre', 'r2', 'rmse', 'mae']]

print(comparativa)

# Gráfica comparativa de métricas
fig, axes = plt.subplots(1, 3, figsize=(15, 5))

métricas = ['r2', 'rmse', 'mae']
títulos = ['R² (Más alto mejor)', 'RMSE (Más bajo mejor)', 'MAE (Más bajo mejor)']

for i, (metrica, titulo) in enumerate(zip(métricas, títulos)):
    axes[i].bar(comparativa['nombre'], comparativa[metrica], color=['skyblue', 'lightcoral'])
    axes[i].set_title(titulo)
    axes[i].set_ylabel(metrica.upper())

    for j, v in enumerate(comparativa[metrica]):
        axes[i].text(j, v + 0.01, f'{v:.4f}', ha='center', va='bottom')

plt.tight_layout()
plt.show()

"""Los resultados demuestran que el modelo entrenado excluyendo el período COVID (2020-2021) presenta mejor desempeño predictivo:

- **R² más alto** (0.9145 vs 0.8954): Explica aproximadamente 2% más de la varianza
- **RMSE y MAE más bajos**: Errores de predicción menores en 0.0221 y 0.0215 puntos respectivamente

El COVID representó una excepción en los datos demográficos, introduciendo ruido y comportamientos atípicos que dificultan encontrar patrones fijos. Al excluir estos datos, el modelo podrá ver mejor las tendencias importantes y a largo plazo de la natalidad peruana.

## **8. Análisis de Importancia de Variables**

### **8.1. Importancia de características del modelo XGBoost**
"""

# Obtener importancia de características
importancia = xgb_final.feature_importances_
variables = X_train_full.columns

# Crear DataFrame con las importancias
df_importancia = pd.DataFrame({
    'Variables': variables,
    'Importancia': importancia
}).sort_values('Importancia', ascending=False)

# Visualizar top 15 variables más importantes
plt.figure(figsize=(12, 8))
sns.barplot(data=df_importancia.head(15), y='Variables', x='Importancia', palette='viridis')
plt.title('Top 15 - Variables Más Importantes para Predecir Tasa de Natalidad\n(Modelo XGBoost)')
plt.xlabel('Importancia Relativa')
plt.tight_layout()
plt.show()

print("TOP 10 VARIABLES MÁS INFLUYENTES:\n")
for i, row in df_importancia.head(10).iterrows():
    print(f"{row['Variables']}: {row['Importancia']:.4f}")

"""### **8.2. Análisis de importancia por categorías de variables**"""

# Clasificar variables en categorías
categorias = {
    'Temporal': ['lag1', 'lag4', 'ma4', 'Anho', 'Trimestre'],
    'Edad_Madre': ['Edad_Media', 'Pct_Adolescentes', 'Pct_Mayores35'],
    'Reproductivo': ['Embarazos_Prom', 'HijosVivos_Prom', 'Abortos_Prom'],
    'Educacion': ['Pct_Nivel_SUPERIOR', 'Pct_Nivel_SECUNDARIA', 'Pct_Nivel_PRIMARIA_BAJA'],
    'Estado_Civil': ['Pct_EstadoCivil_CON_PAREJA'],
    'Atencion_Salud': ['Pct_Lugar_INSTITUCIONAL', 'Pct_Financ_SIS', 'Pct_Financ_ESSALUD',
                      'Pct_Financ_SANIDADES', 'Pct_Financ_PRIVADO'],
    'Geografico': [col for col in df_importancia['Variables'] if 'Departamento_' in col]
}

# Calcular importancia por categoría
importancia_categorias = {}
for categoria, vars_categoria in categorias.items():
    importancia_cat = df_importancia[df_importancia['Variables'].isin(vars_categoria)]['Importancia'].sum()
    importancia_categorias[categoria] = importancia_cat

# Visualizar importancia por categorías
df_importancia_cat = pd.DataFrame({
    'Categoria': list(importancia_categorias.keys()),
    'Importancia': list(importancia_categorias.values())
}).sort_values('Importancia', ascending=False)

plt.figure(figsize=(10, 6))
sns.barplot(data=df_importancia_cat, y='Categoria', x='Importancia', palette='Set2')
plt.title('Importancia de Variables por Categorías')
plt.xlabel('Importancia Acumulada')
plt.tight_layout()
plt.show()

print("IMPORTANCIA POR CATEGORÍAS:\n")
for cat, imp in importancia_categorias.items():
    print(f"{cat}: {imp:.4f}")

"""### **8.3. Análisis de patrones geográficos**"""

# Extraer importancia de variables departamentales
imp_deptos = df_importancia[df_importancia['Caracteristica'].str.contains('Departamento_')].copy()
imp_deptos['Departamento'] = imp_deptos['Caracteristica'].str.replace('Departamento_', '')

print("TOP 10 DEPARTAMENTOS CON MAYOR INFLUENCIA:\n")
for i, row in imp_deptos.head(10).iterrows():
    print(f"{row['Departamento']}: {row['Importancia']:.4f}")

# Visualizar importancia de departamentos
plt.figure(figsize=(12, 8))
sns.barplot(data=imp_deptos.head(15), y='Departamento', x='Importancia', palette='coolwarm')
plt.title('Importancia de Variables Departamentales en la Predicción')
plt.xlabel('Importancia Relativa')
plt.tight_layout()
plt.show()

"""### **8.4. Análisis profundo de departamentos clave**"""

# Analizar características únicas de los departamentos más influyentes
deptos_clave = ['HUANCAVELICA', 'MADRE DE DIOS', 'AYACUCHO', 'TUMBES', 'HUANUCO']

# Crear dataset filtrado para análisis comparativo
df_deptos_clave = df_final[df_final['Departamento'].isin(deptos_clave)].copy()

# Calcular promedios nacionales (excluyendo departamentos clave para comparación)
df_otros_deptos = df_final[~df_final['Departamento'].isin(deptos_clave)]
promedio_nacional = df_otros_deptos[['Tasa_Natalidad_Trimestral', 'Edad_Media', 'Pct_Adolescentes',
                                   'Pct_Nivel_SUPERIOR', 'Pct_Financ_SIS']].mean()

# Calcular datos nacionales
print(f"NACIONAL:")
print(f"  - Tasa Natalidad: {promedio_nacional['Tasa_Natalidad_Trimestral']:.2f}")
print(f"  - Edad Media Madre: {promedio_nacional['Edad_Media']:.1f} años")
print(f"  - % Adolescentes: {promedio_nacional['Pct_Adolescentes']:.1%}")
print(f"  - % Educación Superior: {promedio_nacional['Pct_Nivel_SUPERIOR']:.1%}")
print(f"  - % SIS: {promedio_nacional['Pct_Financ_SIS']:.1%}")

print(f"\nPOR DEPARTAMENTO CLAVE:")
# Analizar cada departamento clave
for depto in deptos_clave:
    datos_depto = df_final[df_final['Departamento'] == depto]

    print(f"\n  {depto}:")
    print(f"    - Tasa Natalidad: {datos_depto['Tasa_Natalidad_Trimestral'].mean():.2f}")
    print(f"    - Edad Media Madre: {datos_depto['Edad_Media'].mean():.1f} años")
    print(f"    - % Adolescentes: {datos_depto['Pct_Adolescentes'].mean():.1%}")
    print(f"    - % Educación Superior: {datos_depto['Pct_Nivel_SUPERIOR'].mean():.1%}")
    print(f"    - % SIS: {datos_depto['Pct_Financ_SIS'].mean():.1%}")

    # Calcular volatilidad (desviación estándar de la tasa)
    volatilidad = datos_depto['Tasa_Natalidad_Trimestral'].std()
    print(f"    - Volatilidad Tasa: {volatilidad:.2f} (indicador de variabilidad temporal)")

# Visualizar comparativa de tasas de natalidad
plt.figure(figsize=(12, 6))
deptos_data = []
for depto in deptos_clave:
    deptos_data.append(df_final[df_final['Departamento'] == depto]['Tasa_Natalidad_Trimestral'].mean())

plt.bar(deptos_clave, deptos_data, color=['orange', 'lightgreen', 'skyblue', 'violet', 'pink'])
plt.axhline(y=promedio_nacional['Tasa_Natalidad_Trimestral'], color='red', linestyle='--',
           label=f'Promedio Nacional: {promedio_nacional["Tasa_Natalidad_Trimestral"]:.2f}')
plt.ylabel('Tasa de Natalidad Promedio')
plt.title('Comparación de Tasas de Natalidad - Departamentos Clave vs Nacional')
plt.legend()
plt.tight_layout()
plt.show()

"""Los departamentos que más influyen en el modelo presentan tasas de natalidad notablemente más altas que el promedio nacional (3.56), especialmente Huancavelica y Huánuco (4.82). Este perfil se relaciona consistentemente con:
*  Madres más jóvenes: hasta 1.2 años menos que el promedio nacional
*  Mayor maternidad adolescente: Huancavelica 17.2% vs 12.1% nacional
*  Menor educación superior: Huancavelica 21.2% vs 32.8% nacional
*  Alta dependencia del SIS: hasta 88.8% en Huancavelica

La fuerte influencia de estos departamentos no solo viene de sus características estructurales, sino también de su mayor variabilidad temporal:
*  Tumbes tiene la mayor volatilidad (0.74) a pesar de indicadores educativos similares al promedio
*  Ayacucho combina alta natalidad con alta variabilidad (0.55)
*  Esta inestabilidad genera "señales más fuertes" para el aprendizaje del modelo

Los departamentos con transiciones demográficas más dinámicas y mayores desigualdades socioeconómicas aportan más información predictiva. Esto indica que la heterogeneidad regional no es ruido, sino una señal valiosa para entender los factores detrás de la natalidad peruana.

Finalmente, emerge un patrón de "departamentos predictores" con economías en transición, acceso limitado a educación superior y sistemas de salud principalmente públicos, que experimentan cambios demográficos acelerados.
"""